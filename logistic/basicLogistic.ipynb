{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "# Basic Logistic Regression\n",
    "\n",
    "This notebook implements a basic logistic regresion on our RESISC45 image dataset. To simplify implementation,\n",
    "we used the ScikitLearn `LogisticRegression` class.\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/cs449s23/project-6-layers-deep/blob/main/basicLogistic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "## Setup\n",
    "\n",
    "To start, we import everything needed, and download our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5uKYm05SOtAt"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpathlib\u001b[39;00m \u001b[39mimport\u001b[39;00m Path\n\u001b[0;32m----> 4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransforms\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtransforms\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m ImageFolder\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import gdown\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "RESISC45_DIR = Path(\".\") / \"NWPU-RESISC45\"\n",
    "if not RESISC45_DIR.exists():\n",
    "    gdown.download(\n",
    "        id=\"1nd0R9iljzkWd7Hhfyp2tH55KxAsKrzYj\",\n",
    "        output=\"NWPU-RESISC45.rar\",\n",
    "        quiet=False,\n",
    "    )\n",
    "    !unrar x NWPU-RESISC45.rar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create helper functions for training.\n",
    "\n",
    "Currently, the only one needed is a data splitter to get our train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QThRI8_JPir7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# uses sklearn train_test_split to randomly return the desired train test split\n",
    "def split_data(data, train_size=0.8):\n",
    "    \"\"\"Split the data into test and train sets.\n",
    "\n",
    "    Uses sklearn's `train_test_split`.\n",
    "\n",
    "    Returns (train_indices, test_indicies).\n",
    "    \"\"\"\n",
    "    target_array = data.targets\n",
    "\n",
    "    train_indices, test_indices = train_test_split(\n",
    "        range(len(target_array)),\n",
    "        train_size=train_size,\n",
    "        random_state=69,\n",
    "        stratify=target_array,\n",
    "    )\n",
    "\n",
    "    return train_indices, test_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Now, we move on to training our model.\n",
    "\n",
    "### Hyperparameters and Data Wrangling\n",
    "\n",
    "The first step is setting up our hyperparameters and reading our data in.\n",
    "As part of our data pipeline, we resize all images, convert them to tensors,\n",
    "and normalize them. The normalization coefficients were taken from ImageNet.\n",
    "We then split the data into train and test sets, and set up `DataLoader`s to\n",
    "automatically read in, batch, and sample from our data.\n",
    "\n",
    "We then make sure the data is in the proper format for sklearn, which involves converting it to a Numpy array and\n",
    "reshaping it to a 2D array (from the $256 \\times 256 \\times 3$ image we read in)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "WWCP_htOQqzK",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transforms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m image_size \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m)\n\u001b[0;32m      2\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[1;32m----> 4\u001b[0m data_transforms \u001b[38;5;241m=\u001b[39m \u001b[43mtransforms\u001b[49m\u001b[38;5;241m.\u001b[39mCompose(\n\u001b[0;32m      5\u001b[0m     [\n\u001b[0;32m      6\u001b[0m         transforms\u001b[38;5;241m.\u001b[39mResize(image_size),\n\u001b[0;32m      7\u001b[0m         transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[0;32m      8\u001b[0m         transforms\u001b[38;5;241m.\u001b[39mNormalize(\n\u001b[0;32m      9\u001b[0m             [\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m], [\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m]\n\u001b[0;32m     10\u001b[0m         ),  \u001b[38;5;66;03m# normalization\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     ]\n\u001b[0;32m     12\u001b[0m )\n\u001b[0;32m     14\u001b[0m data \u001b[38;5;241m=\u001b[39m ImageFolder(RESISC45_DIR, transform\u001b[38;5;241m=\u001b[39mdata_transforms)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# get indices of split\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'transforms' is not defined"
     ]
    }
   ],
   "source": [
    "image_size = (256, 256)\n",
    "batch_size = 32\n",
    "\n",
    "data_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "        ),  # normalization\n",
    "    ]\n",
    ")\n",
    "\n",
    "data = ImageFolder(RESISC45_DIR, transform=data_transforms)\n",
    "\n",
    "# get indices of split\n",
    "train_indices, test_indices = split_data(data)\n",
    "\n",
    "# Create a sampler out of the indices\n",
    "train_sampler = torch.utils.data.sampler.SubsetRandomSampler(train_indices)\n",
    "test_sampler = torch.utils.data.sampler.SubsetRandomSampler(test_indices)\n",
    "\n",
    "# give sampler to dataloader\n",
    "# batch size is the whole train/test set so we can transform it to numpy\n",
    "train_loader = DataLoader(data, batch_size=len(train_indices), sampler=train_sampler)\n",
    "test_loader = DataLoader(data, batch_size=len(test_indices), sampler=test_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jPcwVrH5_zJO"
   },
   "outputs": [],
   "source": [
    "# convert tensors to numpy\n",
    "Xtrain = next(iter(train_loader))[0].numpy()\n",
    "ytrain = next(iter(train_loader))[1].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NMOcWhrCG2v5"
   },
   "outputs": [],
   "source": [
    "# convert tensors to numpy\n",
    "Xtest = next(iter(test_loader))[0].numpy()\n",
    "ytest = next(iter(test_loader))[1].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nEjAoMbOG-Ai"
   },
   "outputs": [],
   "source": [
    "# reshape the data to be 2d array as sklearn logistic does not play well with 3d\n",
    "reshape_Xtrain = Xtrain.reshape(Xtrain.shape[0], -1)\n",
    "reshape_Xtest = Xtest.reshape(Xtest.shape[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Thankfully, sklearn handles a lot of the implementation of the LogisticRegression for us. We\n",
    "simply train and then view the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CP4ifCJESDbd",
    "outputId": "ea9befcd-8d62-47d1-8177-0179a82fc025"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# fit model\n",
    "log_model = LogisticRegression().fit(reshape_Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i-6mh-a7Idnb",
    "outputId": "3647c4ee-8cb6-4578-d2d6-c5d2848262ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.8502777777777778\n",
      "Test Accuracy: 0.017142857142857144\n"
     ]
    }
   ],
   "source": [
    "# 80-20 split results\n",
    "print(f\"Train Accuracy: {accuracy_score(ytrain, log_model.predict(reshape_Xtrain))}\")\n",
    "print(f\"Test Accuracy: {accuracy_score(ytest, log_model.predict(reshape_Xtest))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expirementation\n",
    "\n",
    "The below is the same as above, but we use a 20/80 test/train split (rather than\n",
    "the conventional 80/20) to view overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "KViWF59jn-lh"
   },
   "outputs": [],
   "source": [
    "# everything below this cell is same as above with the change of using 20% train\n",
    "data = ImageFolder(RESISC45_DIR, transform=data_transforms)\n",
    "\n",
    "# get indices of split\n",
    "train_indices, test_indices = split_data(data, train_size=0.2)\n",
    "\n",
    "# Create a sampler out of the indices\n",
    "train_sampler = torch.utils.data.sampler.SubsetRandomSampler(train_indices)\n",
    "test_sampler = torch.utils.data.sampler.SubsetRandomSampler(test_indices)\n",
    "\n",
    "# give sampler to dataloader\n",
    "train_loader = DataLoader(data, batch_size=len(train_indices), sampler=train_sampler)\n",
    "test_loader = DataLoader(data, batch_size=len(test_indices), sampler=test_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t1Vsc_wooDKK",
    "outputId": "b6cc7077-848b-4d62-cea8-5b36a869632c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# convert tensors to numpy\n",
    "Xtrain = next(iter(train_loader))[0].numpy()\n",
    "ytrain = next(iter(train_loader))[1].numpy()\n",
    "Xtest = next(iter(test_loader))[0].numpy()\n",
    "ytest = next(iter(test_loader))[1].numpy()\n",
    "\n",
    "# 2d array\n",
    "reshape_Xtrain = Xtrain.reshape(Xtrain.shape[0], -1)\n",
    "reshape_Xtest = Xtest.reshape(Xtest.shape[0], -1)\n",
    "\n",
    "log_model = LogisticRegression().fit(reshape_Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lynEYls4oJjF",
    "outputId": "759b5fdf-19c7-497f-fc87-e889d54f4505"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9998412698412699\n",
      "Test Accuracy: 0.02158730158730159\n"
     ]
    }
   ],
   "source": [
    "# 20-80 split\n",
    "print(f\"Train Accuracy: {accuracy_score(ytrain, log_model.predict(reshape_Xtrain))}\")\n",
    "print(f\"Test Accuracy: {accuracy_score(ytest, log_model.predict(reshape_Xtest))}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
